version: '3.8'

# =============================================================================
# On-Premise LLM & RAG System - Docker Compose Configuration
# Air-gapped Environment with RBAC, Audit Logging, and Daily NAS Sync
# =============================================================================

services:
  # =============================================================================
  # PostgreSQL - RDBMS for users, audit logs, metadata
  # =============================================================================
  postgres:
    image: postgres:16-alpine
    container_name: onprem_postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-admin}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-securepassword}
      POSTGRES_DB: ${POSTGRES_DB:-onprem_llm}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    networks:
      - onprem_network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-admin}"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # =============================================================================
  # Redis - Cache & Task Queue
  # =============================================================================
  redis:
    image: redis:7.2-alpine
    container_name: onprem_redis
    command: redis-server --appendonly yes --maxmemory 4gb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    networks:
      - onprem_network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    restart: unless-stopped

  # =============================================================================
  # Qdrant - Vector Database
  # =============================================================================
  qdrant:
    image: qdrant/qdrant:v1.7.4
    container_name: onprem_qdrant
    environment:
      QDRANT__SERVICE__HTTP_PORT: 6333
      QDRANT__SERVICE__GRPC_PORT: 6334
    volumes:
      - qdrant_data:/qdrant/storage
    ports:
      - "6333:6333"
      - "6334:6334"
    networks:
      - onprem_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # =============================================================================
  # GLM-OCR Service - Image OCR
  # =============================================================================
  ocr_service:
    build:
      context: ./services/ocr
      dockerfile: Dockerfile
    container_name: onprem_ocr
    environment:
      - OCR_MODEL=zai-org/GLM-OCR
      - OCR_MODEL_PATH=${OCR_MODEL_PATH:-zai-org/GLM-OCR}
      - OCR_SERVICE_PORT=8001
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ${MODEL_CACHE_DIR:-./model_cache}/ocr:/root/.cache/huggingface
    ports:
      - "8001:8001"
    networks:
      - onprem_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    restart: unless-stopped

  # =============================================================================
  # E5 Embedding Service - Text Embeddings
  # =============================================================================
  embedding_service:
    build:
      context: ./services/embedding
      dockerfile: Dockerfile
    container_name: onprem_embedding
    environment:
      - EMBEDDING_MODEL=intfloat/multilingual-e5-large
      - EMBEDDING_MODEL_PATH=${EMBEDDING_MODEL_PATH:-intfloat/multilingual-e5-large}
      - EMBEDDING_SERVICE_PORT=8002
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ${MODEL_CACHE_DIR:-./model_cache}/embedding:/root/.cache/huggingface
    ports:
      - "8002:8002"
    networks:
      - onprem_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    restart: unless-stopped

  # =============================================================================
  # Hybrid Chunking Service - Text Splitting
  # =============================================================================
  chunking_service:
    build:
      context: ./services/chunking
      dockerfile: Dockerfile
    container_name: onprem_chunking
    environment:
      - CHUNKING_SERVICE_PORT=8003
    ports:
      - "8003:8003"
    networks:
      - onprem_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # =============================================================================
  # vLLM Service - LLM Inference
  # Choose ONE scenario by uncommenting the appropriate section
  # =============================================================================

  # --- SCENARIO A: 1 GPU (TP=1) ---
  # Use for: Development, Testing, Single-User, Models up to 70B (quantized)
  # Expected throughput: 15-20 req/s with continuous batching
  vllm_service:
    build:
      context: ./vllm
      dockerfile: Dockerfile.1gpu
    container_name: onprem_vllm_1gpu
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_TENSOR_PARALLEL_SIZE=1
      - MODEL_NAME=${MODEL_NAME:-meta-llama/Llama-2-70b-chat-hf}
      - MODEL_PATH=/models
      - MAX_MODEL_LEN=4096
      - GPU_MEMORY_UTILIZATION=0.90
      - TRUST_REMOTE_CODE=true
      - MAX_NUM_SEQS=256
      - ENABLE_CHUNKED_PREFILL=true
    volumes:
      - ${MODEL_DIR:-/mnt/models}:/models:ro
    ports:
      - "8001:8000"
    networks:
      - onprem_network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    shm_size: "8gb"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    restart: unless-stopped

  # --- SCENARIO B: 2 GPUs (TP=2) ---
  # Uncomment below and comment out Scenario A
  # Use for: Larger models (70B+), Lower latency per request
  # Expected throughput: 30-40 req/s with tensor parallelism
  # vllm_service:
  #   build:
  #     context: ./vllm
  #     dockerfile: Dockerfile.2gpu
  #   container_name: onprem_vllm_2gpu
  #   environment:
  #     - CUDA_VISIBLE_DEVICES=0,1
  #     - VLLM_TENSOR_PARALLEL_SIZE=2
  #     - MODEL_NAME=${MODEL_NAME:-meta-llama/Llama-2-70b-chat-hf}
  #     - MODEL_PATH=/models
  #     - MAX_MODEL_LEN=4096
  #     - GPU_MEMORY_UTILIZATION=0.90
  #     - TRUST_REMOTE_CODE=true
  #     - MAX_NUM_SEQS=512
  #     - ENABLE_CHUNKED_PREFILL=true
  #   volumes:
  #     - ${MODEL_DIR:-/mnt/models}:/models:ro
  #   ports:
  #     - "8001:8000"
  #   networks:
  #     - onprem_network
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 2
  #             capabilities: [gpu]
  #   shm_size: "16gb"
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 120s
  #   restart: unless-stopped

  # --- SCENARIO C: 4 GPUs (TP=4) ---
  # Uncomment below and comment out Scenario A
  # Use for: Very large models (405B), Maximum throughput
  # Expected throughput: 50+ req/s with tensor parallelism
  # vllm_service:
  #   build:
  #     context: ./vllm
  #     dockerfile: Dockerfile.4gpu
  #   container_name: onprem_vllm_4gpu
  #   environment:
  #     - CUDA_VISIBLE_DEVICES=0,1,2,3
  #     - VLLM_TENSOR_PARALLEL_SIZE=4
  #     - MODEL_NAME=${MODEL_NAME:-meta-llama/Llama-2-405b-chat-hf}
  #     - MODEL_PATH=/models
  #     - MAX_MODEL_LEN=8192
  #     - GPU_MEMORY_UTILIZATION=0.95
  #     - TRUST_REMOTE_CODE=true
  #     - MAX_NUM_SEQS=1024
  #     - ENABLE_CHUNKED_PREFILL=true
  #   volumes:
  #     - ${MODEL_DIR:-/mnt/models}:/models:ro
  #   ports:
  #     - "8001:8000"
  #   networks:
  #     - onprem_network
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 4
  #             capabilities: [gpu]
  #   shm_size: "32gb"
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #     start_period: 180s
  #   restart: unless-stopped

  # =============================================================================
  # FastAPI Backend
  # =============================================================================
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: onprem_backend
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4 --loop uvloop --http httptools
    environment:
      - ENVIRONMENT=production
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-admin}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-securepassword}
      - POSTGRES_DB=${POSTGRES_DB:-onprem_llm}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - QDRANT_COLLECTION_NAME=documents
      - VLLM_URL=http://vllm_service:8000
      - JWT_SECRET_KEY=${JWT_SECRET_KEY:-your-secret-key-change-in-production}
      - CORS_ORIGINS=http://localhost:3000
      - MAX_CONCURRENT_REQUESTS=50
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-sentence-transformers/all-MiniLM-L6-v2}
    volumes:
      - ${NAS_MOUNT_PATH:-/mnt/nas}:/mnt/nas:ro
      - backend_logs:/app/logs
      - embeddings_cache:/root/.cache/huggingface
    ports:
      - "8000:8000"
    networks:
      - onprem_network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      vllm_service:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # =============================================================================
  # Celery Worker - Document Processing & Background Tasks
  # =============================================================================
  celery_worker:
    build:
      context: ./worker
      dockerfile: Dockerfile
    container_name: onprem_celery_worker
    command: celery -A celery_app worker --loglevel=info --concurrency=4 --pool=prefork
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-admin}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-securepassword}
      - POSTGRES_DB=${POSTGRES_DB:-onprem_llm}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - QDRANT_COLLECTION_NAME=documents
      - VLLM_URL=http://vllm_service:8000
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-sentence-transformers/all-MiniLM-L6-v2}
      - NAS_MOUNT_PATH=${NAS_MOUNT_PATH:-/mnt/nas}
      - OCR_ENGINE=tesseract
      - TESSERACT_LANG=eng+kor
    volumes:
      - ${NAS_MOUNT_PATH:-/mnt/nas}:/mnt/nas:rw
      - worker_logs:/app/logs
      - embeddings_cache:/root/.cache/huggingface
    networks:
      - onprem_network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    restart: unless-stopped

  # =============================================================================
  # Celery Beat - Task Scheduler (Daily NAS Sync at 02:00 AM)
  # =============================================================================
  celery_beat:
    build:
      context: ./worker
      dockerfile: Dockerfile
    container_name: onprem_celery_beat
    command: celery -A celery_app beat --loglevel=info --scheduler django_celery_beat.schedulers:DatabaseScheduler
    environment:
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_USER=${POSTGRES_USER:-admin}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-securepassword}
      - POSTGRES_DB=${POSTGRES_DB:-onprem_llm}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - NAS_SYNC_SCHEDULE=${NAS_SYNC_SCHEDULE:-0 2 * * *}  # Daily at 02:00 AM (cron format)
    volumes:
      - beat_schedule:/app/schedule
    networks:
      - onprem_network
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped

  # =============================================================================
  # Flower - Celery Monitoring (Optional, for development)
  # =============================================================================
  flower:
    build:
      context: ./worker
      dockerfile: Dockerfile
    container_name: onprem_flower
    command: celery -A celery_app flower --port=5555 --broker=redis://redis:6379/0
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - FLOWER_BASIC_AUTH=${FLOWER_USER:-admin}:${FLOWER_PASSWORD:-admin}
    ports:
      - "5555:5555"
    networks:
      - onprem_network
    depends_on:
      redis:
        condition: service_healthy
    profiles:
      - dev  # Only starts with: docker compose --profile dev up
    restart: unless-stopped

  # =============================================================================
  # Next.js Frontend - Chat Interface & Admin Dashboard
  # =============================================================================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: onprem_frontend
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000
      - NEXT_PUBLIC_WS_URL=ws://localhost:8000
      - NODE_ENV=production
    ports:
      - "3000:3000"
    networks:
      - onprem_network
    depends_on:
      - backend
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

# =============================================================================
# Networks
# =============================================================================
networks:
  onprem_network:
    driver: bridge
    # Uncomment below for air-gapped environment (no external internet access)
    # internal: true

# =============================================================================
# Volumes - Data Persistence
# =============================================================================
volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  qdrant_data:
    driver: local
  backend_logs:
    driver: local
  worker_logs:
    driver: local
  beat_schedule:
    driver: local
  embeddings_cache:
    driver: local
