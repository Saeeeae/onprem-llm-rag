# vLLM Server Configuration
# Optimized for 100 concurrent requests with Korean LLM

# Model Settings
model:
  name: "llama-3-korean-20b"  # Or "upstage/solar-10.7b-instruct-v1.0"
  trust_remote_code: true
  dtype: "auto"  # Will use bfloat16 on A100

# Parallelism
tensor_parallel_size: 4  # 4x A100 80GB
pipeline_parallel_size: 1

# Memory Management
gpu_memory_utilization: 0.90  # Leave 10% for KV cache overhead
max_model_len: 8192  # Maximum context length
swap_space: 16  # GB, for swapping KV cache to CPU

# Batching Configuration
# Key for high concurrency
max_num_seqs: 256  # Maximum sequences in a batch
max_num_batched_tokens: 32768  # Maximum tokens per batch
max_paddings: 256

# PagedAttention Settings
block_size: 16
enable_chunked_prefill: true  # Better for long prompts

# Quantization (optional, for smaller GPUs)
# quantization: "awq"  # or "gptq", "squeezellm"

# Scheduling
scheduler_delay_factor: 0.0
enable_prefix_caching: true  # Cache common prefixes

# Request handling
max_log_len: 100
disable_log_requests: true  # Reduce logging overhead in production

# Speculative decoding (optional)
# speculative_model: "llama-3-1b-draft"
# num_speculative_tokens: 5
